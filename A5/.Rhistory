library(tidyverse)
library(caret)
data("BreastCancer", package = "mlbench")
bc_data <- BreastCancer[complete.cases(BreastCancer),]
summary(bc_data$Class)
ï¼Ÿcomplete.cases
?complete.cases
set.seed(42)
index <- createDataPartition(bc_data$Class, p = 0.7, list = FALSE)
train_data <- bc_data[index, ] %>% select(-Id)
test_data  <- bc_data[-index, ] %>% select(-Id)
grid = expand.grid(C = c(0.8, 1, 1.2, 2), gamma = c(1, 5, 10))
# baseline by rf in Caret
model_rf <- train(Class ~ ., data = train_data, method = "rf",
trControl = trainControl(method = "cv", number = 5))
final <- data.frame(actual = test_data$Class, predict(model_rf, test_data))
cm_original <- confusionMatrix(final$predict, test_data$Class)
cm_original
ctrl <- trainControl(method = "cv", number = 5, sampling = "down") # for over-sampling, you change down to up
set.seed(42)
model_rf_under <- train(Class ~ ., data = train_data, method = "rf",
trControl = ctrl)
final_under <- data.frame(actual = test_data$Class, predict(model_rf_under, test_data))
cm_under <- confusionMatrix(final_under$predict, test_data$Class)
# SMOTE by Caret
ctrl <- trainControl(method = "cv", number = 5, sampling = "smote")
set.seed(42)
model_rf_smote <- train(Class ~ ., data = train_data, method = "rf", trControl = ctrl)
final_smote <- data.frame(actual = test_data$Class, predict(model_rf_smote, test_data))
cm_smote <- confusionMatrix(final_smote$predict, test_data$Class)
library(e1071)
library(caret)
train <- read.csv("data/A3_train.csv", colClasses = append(rep(c("numeric"), times = 15), "factor"))
cols <- c('x4', 'x8', 'y')
train_t <- train[1:1500, cols]
train_v <- train[1501:2000, cols]
# Plot to see how data look like
ggplot(train_t, aes(x = x4, y = x8, color = y)) + geom_point(shape = 1) + ggtitle("training")
ggplot(train_v, aes(x = x4, y = x8, color = y)) + geom_point(shape = 1) + ggtitle("validation")
#linear kernel
model.linear <- svm(y ~ ., data=train_t, kernel = 'linear', cost = 1)
plot(model.linear, train_t)
pred.linear <-  predict(model.linear, train_v)
confusionMatrix(pred.linear, train_v$y)
#Polynomial Kernel
model.poly <- svm(y ~ ., data=train_t, kernel = 'polynomial', gama=1,cost=1)#train
tune.poly <- tune(svm,y~.,data=train_t,ranges=list(cost=c(.0001,.001,.01,.1,10,100,1000),gamma=c(.01,.1,.5,1,2,3,4),degree=c(2,3,4)))#tune
library(caret)
set.seed(5152)
data <- read.csv("data/application_v3.csv")
data$TARGET <- factor(data$TARGET)
levels(data$TARGET) <- c("OTHER", "DIFFICULTY")
train_idx <- createDataPartition(data$TARGET, p = 0.7, list = FALSE)
train <- data[train_idx, ]
test  <- data[-train_idx, ]
ctrl_base <- trainControl(method = "cv", number = 5)
model.baseline <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "ROC", trControl = ctrl_base)
ctrl_base <- trainControl(method = "cv", number = 5,classProbs = TRUE)
model.baseline <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "ROC", trControl = ctrl_base)
View(train)
ctrl_base <- trainControl(method = "cv", number = 5,classProbs = TRUE)
model.baseline <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "ROC", trControl = ctrl_base)
ctrl_base <- trainControl(method = "cv", number = 5,)
model.baseline <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_base)
#using XGBoost as the baseline model to predict "TARGET"
ctrl_base <- trainControl(method = "cv", number = 5)
model.baseline <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_base)
pred.base <- predict(model.baseline, test)
cm.base <- confusionMatrix(pred.base, test$TARGET)
ctrl_under <- trainControl(method = "cv", number = 5, sampling = "down") # for over-sampling, you change down to up
# values supported in Caret sampling are "none", "down", "up", "smote", or "rose"
model.under <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_under)
pred.under <- predict(model.under, test)
cm.under <- confusionMatrix(pred.under, test$TARGET)
#Over-sampling by Caret
ctrl_over <- trainControl(method = "cv", number = 5, sampling = "up")
# for over-sampling, you change down to up
# values supported in Caret sampling are "none", "down", "up", "smote", or "rose"
model.over <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "ROC", trControl = ctrl_over)
ctrl_over <- trainControl(method = "cv", number = 5, sampling = "up")
# for over-sampling, you change down to up
# values supported in Caret sampling are "none", "down", "up", "smote", or "rose"
model.over <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_over)
student.name <- "LI LIPING"  # put your name here
student.id <- 0320278  # put only the numeric digits of your NUS user id here
set.seed(student.id)
load("sales.rdata")  # load the data (make sure the .rdata file is in your working directory!)
ls(all.names=T)  # list all the objects in the data file
summary(data.train)
summary(data.test)
names(data.train)
options(width = 100)  # set output width
student.name <- "LI LIPING"  # put your name here
student.id <- 0320278  # put only the numeric digits of your NUS user id here
set.seed(student.id)
load("sales.rdata")  # load the data (make sure the .rdata file is in your working directory!)
ls(all.names=T)  # list all the objects in the data file
summary(data.train)
summary(data.test)
formula_1a <- as.formula(sales~weekend+holiday+prod_function+prod_regime+prod_listprice+discount+stockout)
formula_1b <- as.formula(sales~weekend+holiday+prod_function+prod_regime+prod_listprice+discount)
formula_1c <- as.formula(sales~weekend+holiday+prod_function+prod_listprice+discount)
data.train$log_sales <- log(1 + data.train$sales)
data.test$log_sales <- log(1 + data.test$sales)
#define a formula for following questions
formula <- as.formula(log_sales ~ weekend + holiday + prod_function + prod_regime + prod_listprice + discount + stockout)
#Fit a linear regression model
model_lm <- lm(formula,data=data.train)
#generate predictions on both the training and the test datasets
pred_lm_train <- predict(model_lm,data.train)
pred_lm_test <- predict(model_lm,data.test)
library("glmnetUtils")
### The Lasso model
# glmnet with alpha=1 means LASSO
model_lasso <- glmnet(formula,data.train,alpha=1, use.model.frame=TRUE)
# CV for optimal lambda
lasso.cv <- cv.glmnet(formula,data.train,alpha=1, use.model.frame=TRUE)
# prediciton on training and test dataset using optimal lambda
pred_lasso_train <- predict(lasso.cv, s=lasso.cv$lambda.min, newdata=data.train, exact=TRUE)
pred_lasso_test <- predict(lasso.cv, s=lasso.cv$lambda.min, newdata=data.test, exact=TRUE)
library("randomForest")
#Fit a Random Forest model
model_rf <- randomForest(formula, data.train)
#generate predictions on both the training and the test datasets
pred_rf_train <- predict(model_rf, newdata=data.train)
pred_rf_test <- predict(model_rf, newdata=data.test)
library(Metrics)
RMSE_lm <- rmse(data.test$log_sales,pred_lm_test)
print("RMSE_lm:",RMSE_lm,"\n")
print("RMSE_lm:",RMSE_lm)
library(Metrics)
RMSE_lm <- rmse(data.test$log_sales,pred_lm_test)
cat("RMSE_lm:",RMSE_lm,"\n")
RMSE_lasso <- rmse(data.test$log_sales,pred_lasso_test)
cat("RMSE_lasso:",RMSE_lasso,"\n")
RMSE_rf <- rmse(data.test$log_sales,pred_rf_test)
cat("RMSE_rf:",RMSE_rf,"\n")
#create an empty data frame to collect the predictions of test data
pred_test <- setNames(as.data.frame(matrix(nrow = nrow(data.test), ncol = 100)),c(1:100))
#a loop to run the bootstrap models
for(i in 1:100){
n_samples <- nrow(data.train)
sample_row_ids <- sample(1:n_samples,n_samples, replace=TRUE)
new.data.train <- data.train[sample_row_ids, ]
lm <- lm(formula, new.data.train)
pred_test[,i] <- predict(lm,data.test)
}
pred_test_bootstrap <- rowMeans(pred_test)
RMSE_bootstrap <- rmse(data.test$log_sales,pred_test_bootstrap)
cat("RMSE_bootstrap:",RMSE_bootstrap,"\n")
#calculate the residuals of the trained RF model on the training data
data.train$residuals <- data.train$log_sales-pred_rf_train
# fit a lasso model on residuals
model_lasso_residuals <- glmnet(residuals ~ weekend + holiday + prod_function + prod_regime + prod_listprice + discount + stockout,data.train,alpha=1, use.model.frame=TRUE)
# CV for optimal lambda
lasso.cv.residuals <- cv.glmnet(residuals ~ weekend + holiday + prod_function + prod_regime + prod_listprice + discount + stockout,data.train,alpha=1, use.model.frame=TRUE)
# prediciton using optimal lambda on test data
pred_residuals <- predict(lasso.cv.residuals, s=lasso.cv.residuals$lambda.min, newdata=data.test, exact=TRUE)
#using the two models together
pred_test_3b <- pred_residuals+pred_rf_test
RMSE_2models <- rmse(data.test$log_sales,pred_test_3b)
cat("RMSE_2models:",RMSE_2models,"\n")
#create two dataframe for stacking
train_stack <- setNames(as.data.frame(matrix(nrow = nrow(data.train), ncol = 4)),c("log_sales","lm","lasso","rf"))
train_stack$log_sales <- data.train$log_sales
train_stack$lm <- pred_lm_train
train_stack$lasso <- pred_lasso_train
train_stack$rf <- pred_rf_train
test_stack <- setNames(as.data.frame(matrix(nrow = nrow(data.test), ncol = 4)),c("log_sales","lm","lasso","rf"))
test_stack$log_sales <- data.test$log_sales
test_stack$lm <- pred_lm_test
test_stack$lasso <- pred_lasso_test
test_stack$rf <- pred_rf_test
# glmnet with alpha=1 means LASSO
model_lasso_3c <- glmnet(log_sales~lm+lasso+rf,train_stack,alpha=1, use.model.frame=TRUE)
# CV for optimal lambda
lasso.cv.3c <- cv.glmnet(log_sales~lm+lasso+rf,train_stack,alpha=1,use.model.frame=TRUE)
# prediciton using optimal lambda
pred_test_3c <- predict(model_lasso_3c, s=lasso.cv.3c$lambda.min, newdata=test_stack, exact=TRUE)
RMSE_stack <- rmse(data.test$log_sales,pred_test_3c)
cat("RMSE_stack:",RMSE_stack,"\n")
options(width = 100)  # set output width
student.name <- "LI LIPING"  # put your name here
student.id <- 0320278  # put only the numeric digits of your NUS user id here
set.seed(student.id)
load("sales.rdata")  # load the data (make sure the .rdata file is in your working directory!)
ls(all.names=T)  # list all the objects in the data file
summary(data.train)
summary(data.test)
formula_1a <- as.formula(sales~weekend+holiday+prod_function+prod_regime+prod_listprice+discount+stockout)
#define a formula for following questions
formula <- as.formula(log_sales ~ weekend + holiday + prod_function + prod_regime + prod_listprice + discount + stockout)
#Fit a linear regression model
model_lm <- lm(formula,data=data.train)
options(width = 100)  # set output width
student.name <- "LI LIPING"  # put your name here
student.id <- 0320278  # put only the numeric digits of your NUS user id here
set.seed(student.id)
load("sales.rdata")  # load the data (make sure the .rdata file is in your working directory!)
View(data.train)
names(data.train)
ls(all.names=T)  # list all the objects in the data file
ls(all.names=T)  # list all the objects in the data file
ls(all.names=T)  # list all the objects in the data file
ls(all.names=T)  # list all the objects in the data file
ls(all.names=T)  # list all the objects in the data file
ls(all.names=T)  # list all the objects in the data file
options(width = 100)  # set output width
student.name <- "LI LIPING"  # put your name here
student.id <- 0320278  # put only the numeric digits of your NUS user id here
set.seed(student.id)
load("sales.rdata")  # load the data (make sure the .rdata file is in your working directory!)
ls(all.names=T)  # list all the objects in the data file
ls(all.names=T)  # list all the objects in the data file
ls(all.names=T)  # list all the objects in the data file
ls(all.names=T)  # list all the objects in the data file
library(caret)
set.seed(5152)
#load data
data <- read.csv("data/application_v3.csv")
data$TARGET <- factor(data$TARGET)
levels(data$TARGET) <- c("OTHER", "DIFFICULTY")
#create the train data and test data
train_idx <- createDataPartition(data$TARGET, p = 0.7, list = FALSE)
train <- data[train_idx, ]
test  <- data[-train_idx, ]
#using XGBoost as the baseline model to predict "TARGET"
ctrl_base <- trainControl(method = "cv", number = 5)
model.baseline <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_base)
pred.base <- predict(model.baseline, test)
cm.base <- confusionMatrix(pred.base, test$TARGET)
# Under-sampling by Caret
ctrl_under <- trainControl(method = "cv", number = 5, sampling = "down")
# values supported in Caret sampling are "none", "down", "up", "smote", or "rose"
model.under <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_under)
pred.under <- predict(model.under, test)
cm.under <- confusionMatrix(pred.under, test$TARGET)
#Over-sampling by Caret
ctrl_over <- trainControl(method = "cv", number = 5, sampling = "up")
# values supported in Caret sampling are "none", "down", "up", "smote", or "rose"
model.over <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_over)
pred.over <- predict(model.over, test)
cm.over <- confusionMatrix(pred.over, test$TARGET)
# SMOTE by Caret
ctrl_smote <- trainControl(method = "cv", number = 5, sampling = "smote")
set.seed(42)
model_smote <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_smote)
pred.smote <- predict(model_smote, test)
cm_smote <- confusionMatrix(pred.smote, test$TARGET)
# comparing performance
models <- list(original = model_rf,
under = model_rf_under,
smote = model_rf_smote
)
models <- list(original = model.baseline,
under = model.under,
over = model.over,
both = model.both,
smote = model.smote
)
# Under-sampling and Over-sampling together by Caret
ctrl_both <- trainControl(method = "cv", number = 5, sampling =c("down","up"))
# values supported in Caret sampling are "none", "down", "up", "smote", or "rose"
model.both <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_over)
pred.both <- predict(model.both, test)
cm.both <- confusionMatrix(pred.both, test$TARGET)
models <- list(original = model.baseline,
under = model.under,
over = model.over,
both = model.both,
smote = model.smote
)
resampling <- resamples(models)
bwplot(resampling)
ctrl_smote <- trainControl(method = "cv", number = 5, sampling = "smote")
set.seed(42)
model.smote <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_smote)
pred.smote <- predict(model.smote, test)
cm.smote <- confusionMatrix(pred.smote, test$TARGET)
ctrl_smote <- trainControl(method = "cv", number = 5, sampling = "smote")
set.seed(42)
model.smote <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_smote)
pred.smote <- predict(model.smote, test)
cm.smote <- confusionMatrix(pred.smote, test$TARGET)
models <- list(original = model.baseline,
under = model.under,
over = model.over,
both = model.both,
smote = model.smote
)
resampling <- resamples(models)
resampling
bwplot(resampling)
cm.base
options(width = 100)  # set output width
student.name <- "LI LIPING"  # put your name here
student.id <- 0320278  # put only the numeric digits of your NUS user id here
set.seed(student.id)
load("sales.rdata")  # load the data (make sure the .rdata file is in your working directory!)
ls(all.names=T)  # list all the objects in the data file
summary(data.train)
summary(data.test)
names(data.train)
library(caret)
set.seed(5152)
#load data
data <- read.csv("data/application_v3.csv")
data$TARGET <- factor(data$TARGET)
levels(data$TARGET) <- c("OTHER", "DIFFICULTY")
#create the train data and test data
train_idx <- createDataPartition(data$TARGET, p = 0.7, list = FALSE)
train <- data[train_idx, ]
test  <- data[-train_idx, ]
#using XGBoost as the baseline model to predict "TARGET"
ctrl_base <- trainControl(method = "cv", number = 5)
model.baseline <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_base)
pred.base <- predict(model.baseline, test)
cm.base <- confusionMatrix(pred.base, test$TARGET)
# Under-sampling by Caret
ctrl_under <- trainControl(method = "cv", number = 5, sampling = "down")
# values supported in Caret sampling are "none", "down", "up", "smote", or "rose"
model.under <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_under)
pred.under <- predict(model.under, test)
cm.under <- confusionMatrix(pred.under, test$TARGET)
#Over-sampling by Caret
ctrl_over <- trainControl(method = "cv", number = 5, sampling = "up")
# values supported in Caret sampling are "none", "down", "up", "smote", or "rose"
model.over <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_over)
pred.over <- predict(model.over, test)
cm.over <- confusionMatrix(pred.over, test$TARGET)
# Under-sampling and Over-sampling together by Caret
ctrl_both <- trainControl(method = "cv", number = 5, sampling =c("down","up"))
# values supported in Caret sampling are "none", "down", "up", "smote", or "rose"
model.both <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_over)
pred.both <- predict(model.both, test)
cm.both <- confusionMatrix(pred.both, test$TARGET)
# SMOTE by Caret
ctrl_smote <- trainControl(method = "cv", number = 5, sampling = "smote")
set.seed(42)
model.smote <- train(TARGET ~ ., data = train, method = "xgbTree", metric = "Accuracy", trControl = ctrl_smote)
pred.smote <- predict(model.smote, test)
cm.smote <- confusionMatrix(pred.smote, test$TARGET)
# comparing performance
models <- list(original = model.baseline,
under = model.under,
over = model.over,
both = model.both,
smote = model.smote
)
resampling <- resamples(models)
bwplot(resampling)
