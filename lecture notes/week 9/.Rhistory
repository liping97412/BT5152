library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
trans <- read.csv('transcripts.csv', stringsAsFactors = FALSE)
main <- read.csv('ted_main.csv', stringsAsFactors = FALSE)
ted <- merge(trans,main,by.x = "url",by.y = "url")
library(tm)
doc_ids <- c(1:2467)
df <- data.frame(doc_id = doc_ids, text = trans$transcript, stringsAsFactors = FALSE)
transcripts <- VCorpus(DataframeSource(df))
# Convert the text to lower case
transcripts <- tm_map(transcripts, content_transformer(tolower))
# Remove numbers
transcripts <- tm_map(transcripts, removeNumbers)
# Remove english common stopwords
transcripts <- tm_map(transcripts, removeWords, stopwords('english'))
# Remove punctuations
transcripts <- tm_map(transcripts, removePunctuation)
# Eliminate extra white spaces
transcripts <- tm_map(transcripts, stripWhitespace)
#stem our words
transcripts <- tm_map(transcripts, stemDocument)
#Create a simple term frequency based DocumentTermMatrix on train_neg as tf_dtm
tf_dtm <- DocumentTermMatrix(transcripts)
set.seed(5152)
library(reshape2)
#obtain the sentiment data from General Inquirer
#install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
require("tm.lexicon.GeneralInquirer")
#store the emotion score in the General Inquirer
emotions <- terms_in_General_Inquirer_categories("EMOT")
#convert dtm to df
tf_df <- as.data.frame(as.matrix(tf_dtm))
#find the words in emotions
emot_words <- tf_df[,which(names(tf_df) %in% emotions)]
# plot the heatmap
heatmap(as.matrix(emot_words))
set.seed(5152)
library(jsonlite)
library(caret)
#replace single quote with double for ratings column
ted$ratings <- gsub("\'","\"", ted$ratings)
#Use the rating with the highest count as the label for each talk
for (i in 1:nrow(ted)){
ted$rating_mode[i] <- fromJSON(ted$ratings[i])$name[which.max(fromJSON(ted$ratings[i])$count)]}
#set an indexed type metadata called rating_mode
meta(transcripts, "rating_mode", type = "indexed") <- ted$rating_mode
#obtain the labels of corpus
label <- as.factor(meta(transcripts)$rating_mode)
#Create DocumentTermMatrix named tf_dtm with transcripts with control
dtm_control <- list(weighting = weightTf)
tf_dtm <- DocumentTermMatrix(transcripts, control = dtm_control)
#make use of words with high frequencies (> 10) as features to the classifier
freq_terms <- findFreqTerms(tf_dtm, lowfreq = 10)
#limit the terms in DocumentTermMatrix to just only these high frequency terms
dtm_control$dictionary <- freq_terms
#re-create tf_dtm with the amended dtm_control
tf_dtm2 <- DocumentTermMatrix(transcripts, control = dtm_control)
#Create a variable features_df to store tf_dtm in data frame
features_df <- tbl_df(data.frame(as.matrix(tf_dtm2)))
#remove columns with near zero variance with nearZeroVar function
nzv_columns <- nearZeroVar(features_df)
#remove all the columns from features_df that has near zero variance
features_df <- features_df[, -nzv_columns]
#seperate the train and test data
train_features <- features_df[1:1500,]
test_features <- features_df[1501:2467,]
train_label <- label[1:1500]
test_label <- label[1501:2467]
#construct the model
ctrl <- trainControl(method = "cv", number = 5)
model <- train(features_df,label, method = "rpart", trControl = ctrl)
#check the predictions of the training data
pred <- predict(model$finalModel, features_df)
#calculate the micro and macro F1
library(performanceEstimation)
classificationMetrics(pred,label)
library(topicmodels)
library(ggplot2)
library(dplyr)
library(tidytext)
train_idx <- sample(tf_dtm$nrow * 0.9)
train <- tf_dtm[train_idx,]
# Note that "test" is used very loosely here and this is not a test dataset
# test is the held-out dataset.
test <- tf_dtm[-train_idx,]
model <- LDA(train, k = 50, method = "Gibbs",control = list(verbose = 1, iter = 100))
# Calculate perplexity
# Since we are using the held-out dataset (test),
# we need to estimate the topic distributions (theta)
perplexity(model, test, use_theta = TRUE, estimate_theta = TRUE)
# Top 10 topics by document
top_topics <- tbl_df(data.frame(t(topics(model, 10))))
# Getting the document-topics distribution for the held-out dataset
test_output <- posterior(model, test)
train_id <- 1:1500
#construct the model
ctrl <- trainControl(method = "cv", number = 5)
model <- train(features_df[train_id,],label[train_id], method = "rpart", trControl = ctrl)
train_id <- 1:round( 0.9*nrow(tf_dtm) )
#construct the model
ctrl <- trainControl(method = "cv", number = 5)
model <- train(features_df[train_id,],label[train_id], method = "rpart", trControl = ctrl)
#check the predictions of the training data
pred <- predict(model$finalModel, features_df[-train_id,])
#calculate the micro and macro F1
library(performanceEstimation)
classificationMetrics(pred,label[-train_id])
