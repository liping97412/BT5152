---
title: "BT5152 A4"
author: "LI LIPING"
date: "17 October 2018"
output: html_document
---
Assignment 4 (Due 30th October 2018 5:59 PM )
###TASK 1
Conduct all relevant data-preprocessing of your textual data (TED Talk Transcripts).You need to describe your steps and processes taken. You may pre-process your dataset once and used it for all the following 3 tasks. [3 marks]
```{r}
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
trans <- read.csv('transcripts.csv', stringsAsFactors = FALSE)
main <- read.csv('ted_main.csv', stringsAsFactors = FALSE)
ted <- merge(trans,main,by.x = "url",by.y = "url")

library(tm)
doc_ids <- c(1:2467)
df <- data.frame(doc_id = doc_ids, text = trans$transcript, stringsAsFactors = FALSE)
transcripts <- VCorpus(DataframeSource(df))
# Convert the text to lower case
transcripts <- tm_map(transcripts, content_transformer(tolower))
# Remove numbers
transcripts <- tm_map(transcripts, removeNumbers)
# Remove english common stopwords
transcripts <- tm_map(transcripts, removeWords, stopwords('english'))
# Remove punctuations
transcripts <- tm_map(transcripts, removePunctuation)
# Eliminate extra white spaces
transcripts <- tm_map(transcripts, stripWhitespace)
#stem our words
transcripts <- tm_map(transcripts, stemDocument)
```
###TASK 2
Using the dictionary approach, determine the emotions that are associated with the
talks by counting the occurrences of the emotions terms. [3 marks]
The final deliverable for this task is a heatmap with one axis as emotions and the other as the talks. The colour of the heatmap will show the count of the emotions.
You can expect it to be very dense and you might not want to show the label for the talks. This visualisation basically helps us to understand what kind of emotions are in the talks.
```{r}
#Create a simple term frequency based DocumentTermMatrix on train_neg as tf_dtm
tf_dtm <- DocumentTermMatrix(transcripts)
#obtain the sentiment data from General Inquirer
install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
require("tm.lexicon.GeneralInquirer")
#store the positive words and negative words in the General Inquirer
positive_words <- terms_in_General_Inquirer_categories("Positiv")
negative_words <- terms_in_General_Inquirer_categories("Negativ")
#Use tm_term_score on tf_dtm with the positive words and negative words
p_scores <- tm_term_score(tf_dtm, positive_words)
n_scores <- tm_term_score(tf_dtm, negative_words)
#calculate the net score
net_scores <- p_scores - n_scores
#create a dataframe for heatmap
doc_ids <- c(1:2467)
trans <- data.frame(doc_id = doc_ids, sentiment = net_scores, stringsAsFactors = FALSE)
#visualization
ggplot(data = trans, aes(x=doc_id, y=sentiment,col=sentiment)) + geom_point()

``` 

```{r}
library(ggplot2)
trans <- read.csv('transcripts.csv', stringsAsFactors = FALSE)
doc_ids <- c(1:2467)
trans <- data.frame(doc_id = doc_ids, text = trans$transcript, stringsAsFactors = FALSE)
tidy_trans <- trans %>%
  group_by(doc_id) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)

library(tidyr)

trans_sentiment <- tidy_trans %>%
  inner_join(get_sentiments("bing")) %>%
  count(doc_id, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(data = trans_sentiment, aes(x=doc_id, y=sentiment,col=sentiment)) + geom_point()


```

###TASK 3
Text Classification: using an appropriate algorithm you deem appropriate, perform a
multi-class classification on the ratings on the talks to predict how the talks will likely to get what rating from the viewers. [3 marks]
For this task, you should evaluate your model based on metrics such as micro and macro F1
and describe what are the common ratings that are misclassified in your model.
Notes:
1. You will need to replace single quote with double for ratings column before using jsonlite to parse it.
2. Use the rating with the highest count as the label for each talk. In other words, you are predicting a categorical variable, NOT a numerical variable such as average
rating.
```{r}
#load data
ted <- read.csv("ted_main.csv")

#replace single quote with double for ratings column
ted$ratings <- gsub("\'","\"", ted$ratings)


```

###TASK 4
Using topic modelling, find the top 10 related talks given a specific talk. You can think of this as sort of related articles feature that is common in many media sites. [3 marks]
You are to show a list of 10 articles for any specified article; you can show a few that have similarity above 0.5. Besides that you should also show your methodology (quantitatively or qualitatively) to derive the optimal topic model and justify your judgement.
Common notes for all tasks For consistency purpose, at the beginning of your script set the random seed to 5152.
```{r}
library(topicmodels)
library(ggplot2)
library(dplyr)
library(tidytext)

transcripts_matrix <-  DocumentTermMatrix(transcripts, control = list(weighting = weightTf))
# Since LDA is an unsupervised algorithm, how do we know what is the optimal number of topics?
# We are going to separate the data to train and held-out dataset
# 90% of the data is used for training

train_idx <- sample(transcripts_matrix$nrow * 0.9)

train <- transcripts_matrix[train_idx,]
# Note that "test" is used very loosely here and this is not a test dataset
# test is the held-out dataset.
test <- transcripts_matrix[-train_idx,]

# Training a single LDA model
# Note: LDA in `topicsmodels` does not take in a TF-IDF DocumentTermMatrix
# Technically, TF-IDF DocumentTermMatrix should yield better distributions
model <- LDA(train, k = 50, method = "Gibbs",control = list(verbose = 1, iter = 100))

# Calculate perplexity
# Since we are using the held-out dataset (test),
# we need to estimate the topic distributions (theta)
perplexity(model, test, use_theta = TRUE, estimate_theta = TRUE)

# Top 5 terms by topic
top_terms <- terms(model, 5)

# Top 5 topics by document
top_topics <- tbl_df(data.frame(t(topics(model, 5))))

# Getting the document-topics distribution for the held-out dataset
test_output <- posterior(model, test)


# Parallel Processing to train multiple LDA models
library(parallel)
library(doParallel)

# Create cluster to run R in parallel; best to use total number of CPU - 1
cl <- makePSOCKcluster(detectCores() - 1)

# Allow libraries such as doParallel and tm to access the cluster
registerDoParallel(cl)

# We are going to train models from 10 to 40 with in multiple of 5
ks <- seq(50, 200, 50)

# Use parSapply to pass in additional parameters to LDA
models <- parSapply(cl = cl, ks,function(k, data) topicmodels::LDA(data, k = k, method = "Gibbs", control = list(iter = 100)),data = train)

# We use the held-out dataset to compute perplexity
# Explain perplexity
perplexities <- parSapply(cl = cl, models,function(m, data) topicmodels::perplexity(m, data, use_theta = TRUE, estimate_theta = TRUE),data = test)

# Getting the index of the model with the lowest perplexity
optimal_idx <- which.min(perplexities)

# Let's plot how the perplexity varies over k
metrics_df <- tbl_df(data.frame(k = ks, perplexity = perplexities))

ggplot(metrics_df) + geom_line(aes(x = k, y = perplexity)) +
  ggtitle("Perplexity over number of topics")

# Now we use tidytext to extract the probability distribution in data frame

topics_terms_df <- tidy(models[[optimal_idx]], matrix = "beta")
documents_topics_df <- tidy(models[[optimal_idx]], matrix = "gamma")

# Plot the top terms in each topic
# Warning: if your number of topics is big, this is will very slow.
topics_terms_df %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# Top 5 terms by topic
top_terms <- terms(models[[optimal_idx]], 5)

# Top topics by document
top_topics <- tbl_df(data.frame(t(topics(models[[optimal_idx]], 5))))

test.topics <- posterior(models[[optimal_idx]], test)

stopCluster(cl)


```
